---
title: "Conditional inference tree"
format: html
---

## Will come in exam: What are the pros and cons of CIT?

# Learning objectives  
Our learning objectives are to:  
  - Understand conditional inference tree (cit) algorithm 
  - Use the ML framework to:  
    - pre-process data
    - train a cit model 
    - evaluate model predictability 
  - Explore a few new concepts:  
    - Iterative search with **simulated annealing** (instead of fixed grid)  
    - Selecting **best model within 1 sd**  
    

# Introduction  
As we previously learned, linear regression models can suffer from **multicollinearity** when two or more predictor variables are highly correlated.  

The methods we mentioned to overcome multicollinearity include:  
  - Dimensionality reduction (e.g., PCA)  
  - Variable selection:
    - by hand [field-specific expertise]
    - by models  

Today, we'll explore another model that performs variable selection, but in a different way: **conditional inference trees (CIT)**.

[ What is porting a model? 
Answer: We are training these models here with intentions that we can make predictions. So, if we want to deploy our model on a dashboard to allow other people to upload their data  to get a prediction, that would be porting our model (now that we have trained our model, we want to make it available for other to upload their data and get a prediction -- that would be porting a model; that would be basically we deploying our model) ]

## CIT 
Conditional inference tree is a recursive model that works iteratively performing 2 steps:  
  - **variable selection**  [i.e., selecting a predictor variable in relation to our predicted variable]
  - **binary split**  
  
It performs **variable selection** by first running all possible bivariate models between the response variable (e.g., strength_gtex) and each individual explanatory variable (e.g., sum_precip.mm_June), in the form of **strength_gtex ~ sum_precip.mm_June**. Then, it selects the explanatory variable with the **lowest p-value** as the most important.  

After it selects the most important variable, it performs a **binary split** on that variable, which involves finding a value of the explanatory variable which, if used to split the data in 2 groups, will minimize the error of the two splits.  

After making the first split, it performs a new iteration on each of the splits, performing again variable selection and binary split.  

[Note: If you imagine this tree could grow indefinitely, until we basically have 1 observation per terminal node (where the tree stops growing -- the bottom nodes with boxplots), this tree can be very very very complex, which will create an issue of overfitting. So we need to find a way to allow this tree to grow just enough, but not excessively enough, we don't want it to overfit. Remember the bias-variance trade-off? This is a model that is very easy to allow it to be too complex. We want to make sure that it's complex enough, but not more complex than needed.]

The tree stops growing (stops iterating) when it reaches a given stopping criteria, as for example, maximum tree depth (= how many layers the tree is gonna have).    


Let's look into how it works, with figures.  

![](https://ars.els-cdn.com/content/image/1-s2.0-S0378429021002331-gr5.jpg)
[Very important note: In conditional inference tree, Whatever grain yield data point on the terminal node that matches a specific predictor variable conditions, if we were to make a prediction based on this model, the prediction will be the mean of the boxplot of that terminal node regardless the yield were higher or lower. As long as the predictor variables match that pattern, the prediction that the model is gonna give you is the mean of the boxplot of that particular terminal node. It's always gonna give you the same predictions even if our predictor variables are slightly changing but still matching the same internal conditions. ]

Terminology:  
  - **Root node**: node on top, with all observations (Rain_Cum)    
  - **Internal node**: all intermediate nodes (e.g., Flag_Leaf_Fungi)    
  - **Leaf/terminal node**: the bottom nodes with the boxplots  
  
Variables selected first (on top) are more important than variables selected afterwards. In this example, the most important variable in explaining grain yield is **Rain_Cum**.  

[Very important note:
Conditional inference tree model is not prone to multicollinearity because even though it is a multivariate analysis, it is doing variable selection on a bivariate relationship. So, it runs all bivariate relationships at each step and never runs a multivariate model in the sense that the predicted variable (e.g., fiber strength) is explained by all the predictor variables (e.g., all 73 weather variables). Instead, the predicted variable (fiber strength) is explained by each individual predictor variable separately, choosing the one with the lowest p-value, and then doing binary split. That is how this Conditional inference tree model remedies multicollinearity. Even though the ensamble of it is a multivariate model, but the individual pieces of the tree are all derived from a bivariate relationship.]

[Is having too many internal/terminal nodes a good thing, or bad thing?
Answer: bad thing, because it will lead to overfitting]

## Creating partitions  
Let's look into a simpler example where we are predicting y as a function of x.  

A simple CIT model from this relationship would be with a single break:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/decision-stump-1.png)

If we make a plot of y ~ x and show the split above along the x axis, this is how it would look like:  

![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/decision-stump-2.png)

We can build a more complex tree by allowing it to be deeper:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/depth-3-decision-tree-1.png)

Which will translate into more breaks along the x-axis of the scatterplot:  

![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/depth-3-decision-tree-2.png)

We can allow it to be VERY complex:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/deep-overfit-tree-1.png)

With the following scatterplot breakpoints:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/deep-overfit-tree-2.png)

So, how can we control the simplicity/complexity of the tree?  

**Training a model by fine-tuning its hyper-parameters**.

There will be 2 main hyperparameters that we will fine-tune:  
  - **maximum depth**: maximum (vertical) depth of the tree (i.e., maximum layer of the tree i.e., how many horizontal layers are stuck on top on each other)
  - **minimum criterion**:  the value of (1 - p-value) that must be exceeded in order to implement a split
     
## Pros vs. cons of CIT  
Pros:  
  - Non-parametric [does not assume any specific distribution of data i.e., no need to satisfy the assumptions of residual normality, independence, equal variance] 
  - It can model non-linear relationships [It can model both linear and non-linear relationships] 
  - The model created is a decision tree, very easy to interpret [One of the biggest pros of CIT (conditional inference tree) model is that it is creating 1 single tree that is highly interpretable. It's a very simple model, and highly interpretable. A lot of time there is a trade-off between a model complexity and model interpretability. Models that are really complex, they may be the best at predicting, but they can also be very difficult to interpret. With CIT (conditional inference tree), the model is really simple so it is easy to interpret.]  
  - Can be used with both numerical and categorical response variables  [as compared to elastic net, which works with only numerical response variable]
  
Cons:  
  - Can have higher bias [very prone to overfitting, but we have to try our best to train it]  
  - Potentially lower predictive power: any observation matching a given condition will be predicted as the mean of the terminal node. [Although on the training set, we see the variability (expressed as error caps) around each of the boxplot at the terminal node, but if we use this model for prediction, each of those conditions will give you a prediction of the *mean* of the boxplot, basically on the prediction it ignores the variability that we see around the error caps/bars of the boxplot]   
  
    
# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("partykit") #one of the packages that implements conditional inference tree on R (there are more packages)
#install.packages("finetune") #to fine tune hyperparameters specific to conditional inference tree model
#install.packages("bonsai") #to fine tune hyperparameters specific to conditional inference tree model

library(tidymodels)
library(tidyverse)
library(vip)
library(partykit)
library(finetune)
library(bonsai)
```

```{r weather}
weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7,
                               strata = strength_gtex #to do stratified sampling on "strength_gtex" to make sure that we have a similar distribution of predicted variable across training and testing 
                               )

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Let's check the distribution of our predicted variable **strength_gtex** across training and testing: 
```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = strength_gtex),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = strength_gtex),
               color = "blue") 
  
```

Now, we put our **test set** aside and continue with our **train set** for training.  

  
### b. Data processing  
Before training, we may need to perform some processing steps, like  
  - normalizing    
  - **removing unimportant variables**  
  - dropping NAs  
  - performing PCA on the go  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

Different model types require different processing steps.  
Let's check what steps are required for an elastic net model (linear_reg).
We can search for that in this link: https://www.tmwr.org/pre-proc-table  [in the Table A.1, we see that the decision_tree() does not require any preprocessing methods (e.g., dummy, zv, impute, decorrelate, normalize, transform). So, we don't have to do any of these preprocessing steps. However, we still want to remove the predictor variables from our data set that are outside of the growing season of cotton. Notice that pre-processing steps differ from model to model, so "recipe' steps will be model-specific.]

> Differently from elastic net, variables do not need to be normalized in conditional inference tree, so we'll skip this step


```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ ., # . is used to indicate all predictor variables should be included in the model (we have 73 predictor variables in this data set) #everything on the left of ~ is/are outcome(s), and everything in the right side of ~ are predictors
         data = weather_train) %>% #note that we are using the training data in this recipe
  # Removing year and site  
    step_rm(year, site, matches("Jan|Feb|Mar|Apr|Nov|Dec")) #%>% #rm in step_rm() stands for remove #although we could have used dplyr::select, but we used step_rm() because it is within the scope of "recipes" #we want to remove year and site from our training set #matches("Jan|Feb|Mar|Apr|Nov|Dec"): to remove weather variables that represent data outside of the growing season: Jan, Feb, Mar, Apr, Nov, Dec  #matches("Jan|Feb|Mar|Apr|Nov|Dec"): to remove all columns that have these months (Jan, Feb, Mar, Apr, Nov, Dec) as part of their (column) names
  # Normalizing all numeric variables except predicted variable
  #step_normalize(all_numeric(), -all_outcomes())

weather_recipe
```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep() #prep(): to prepare the recipe

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use [regression (when response/predicted y variable is numerical) or classification (when response/predicted y variable is categorical)] 

> Elastic nets can only be run for a numerical response variable. CITs can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Conditional inference tree **hyperparameters**:  
  - **tree_depth**: maximum depth of the tree (i.e., maximum layer of the tree; so we are basically allowing the tree to grow to different depths, and see which depth is gonna give us the best model)   
  - **mincriterion**: the value of (1 - p-value) that must be exceeded in order to implement a split  (Let's say, we only want to allow variabels having p-value less than 0.05 to enter the model, then our mincriteroin would be (1 - 0.05) = 0.95 [inverse of p-value] that must be exceeded in order to implement a split)
    - **min_n**: minimum number of data points in a node that are required for the node to be split further [Let's say, we are training our tree, and there is an internal node that has 3 variables in the internal node. If our "min_n" is 5, that node is not allowed to split anymore.]   

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **partykit** engine/package.  

[Note: We determine the model type, then we determine the engine, and (for cit) then we determine the mode. What we mean by engine here is that even though we have a given model type (cit, in this case), there are different packages in R that will do the same analysis. For example, if we use another package called "rpart", that package is very similar to "partykit" (which we are using here), uses R2 as the metric to select variables rather than p-value (as in partykit). So, there are some differences on what fit metrics (e.g., p-value, or R2) each package use to conduct variable selection. In this case, we are using "partykit" engine.]        
  
```{r cit_spec}

cit_spec  <- 
  # Specifying cit as our model type, asking to tune the hyperparameters
  decision_tree(tree_depth = tune()) %>% #tree depth is the hyperparameter we are gonna fine tune, and because we are gonna fine tune, we are not gonna mention any number inside the "tune()" function as a placeholder to tell the training step of this process is to say that hey, we want to fine tune "tree_depth". If we indicated any particular value inside "tune()", it would not fine tune, and would just use that value. E.g., if we indicated 10, it would use 10
    # Specify the engine
  set_engine("partykit", #to specify what package we want to use for the implementation of CIT
             conditional_min_criterion = tune()
             ) %>%
    # Specifying mode
  set_mode("regression") #because CIT can handle both numerical and categorical type of predicted/response variable, we need to specify the mode as "regression" since our predicted/response variable is numerical

#One thing that is different in CIT from elastic net is that the model type was linear_reg() [for linear regression], and both of the hyperparameters that we were fine tuning (penalty, mixture) were at the level of fine type function (i.e., inside linear_regression() function). But for CIT, the "conditional_min_criterion" hyperparameter are specific to the "partykit" engine, because of that we will tell the model to fine tune "conditional_min_criterion" hyperparameter at the set_engine() level instead of the model type level (i.e., inside "decision_tree()" function) as we did in the elastic net
#that's why we need to fine tune this"conditional_min_criterion" hyperparameter inside the set_engine() function by using the "conditional_min_criterion" argument as "conditional_min_criterion = tune()"

cit_spec

```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

> Notice how we have one hyperparameter at the level of decision tree (tree_depth) and another one at the level of the engine (min_criterion).  

### b. Hyper-parameter tuning  
> On our previous exercise, we used a fixed grid search approach. This time, let's use an iterative search approach.

For our iterative search, we need:  
  - Our model specification (`cit_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  
  - **Parameter information** (don't have yet)      
  
[Resampling strategy is basically on your train set -- we use only the train set to find the best combinations of hyperparameters. To do that, we have to further split our train set into analysis set (for training) and assessment set (for validation set) through resmapling methods (e.g., v-fold cross-validation). For each resample, models with given hyperparameter values are fit independently on the analysis set and evaluated on the assessment set. Some common resampling methods are:  V-fold cross-validation, Leave-one-out cross-validation, Bootstrapping 

Let's define our resampling strategy below, using a (v-fold cross-validation, with v = 10) 10-fold cross validation approach:  

```{r resampling_foldcv}
set.seed(34549)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 10)

resampling_foldcv
resampling_foldcv$splits[[1]]
resampling_foldcv$splits[[2]]
```

In the output, we have 10 folds. The whole training data was split into 10 parts, then each one of these folds have 9 parts for training and 1 part for testing. And every single fold has a different part testing i.e., whatever testing part was on the 1st fold, it goes back to the other 9 folds, and 1 of the other 0 folds comes back out to the 2nd fold, and so on.   

On the output for the 1st fold ( resampling_foldcv$splits[[1]] ), notice that 487 is the total number of observations on the 1st fold which is the total number of data in our training set. And 1st fold has 438 rows (of those 487 total rows) are on the (analysis/) training and 49 are on the assessment. Then it does this for all the 10 folds.  

On each fold, we'll use **438** observations for training and **49** observations to assess performance.    

Now, let's define our parameter information.  

We need to create this object because engine-specific hyperparameters have a missing object, and it is required to be not missing to perform simulated annealing.  

```{r cit_param}

#For CIT, we have to do this extra step so we can fine tune the CIT engine (= "partykit") specific hyperparameters: (1) tree_depth , (2) mincriterion, (3) min_n

#tree_depth (maximum depth of the tree i.e., maximum layer of the tree; so we are basically allowing the tree to grow to different depths, and see which depth is gonna give us the best model), mincriterion (the value of (1 - p-value) that must be exceeded in order to implement a split), min_n (minimum number of data points in a node that are required for the node to be split further)

#At this stage: For CIT, we were fine tuning the hyperparameters at the level of the engine, whereas elastic net had all the hyperparameters that we were fine tuning at the level of the model

cit_param <- cit_spec %>%
  extract_parameter_set_dials() %>%
  update(conditional_min_criterion = #"update()" comes from "recipes"
           conditional_min_criterion()) #to set the nparam[+] for the "conditional_min_criterion" which was missing up to the previous pipe #setting for the specification of the model, what are the ranges of the hyperparameters that will be finetuned for each one of those hyperparameters

cit_param 

cit_param$object #We can see what is the range that the model is gonna train for us #From the output, we see that "tree depth" is gonna test for 1 to 15 [i.e., having a tree depth of 1 layer only, to all the way to 15 layers; that's the default it is gonna test for us.]; "Transformer: prob-logis [0, 1]" means that the "conditional_min_criterion" is going from 0 to 1 

```

[Very important note: For the above code chunk: For CIT, we were fine tuning the hyperparameters at the level of the engine, whereas elastic net had all the hyperparameters that we were fine tuning at the level of the model  ]

Now, let's perform the search below.  

[For elastic net, we created a fixed grid at this stage. But for CIT, we will do simulated annealing.]

We will use an iterative search algorithm called **simulated annealing**.  

Here's how it works:  
![](https://www.tmwr.org/figures/iterative-neighborhood-1.png)
  - In the example above, mixture and penalty from an elastic net model are being tuned.  

  - It finds a candidate value of hyperparameters and their associated rmse to start (iteration 1).  

  - It establishes a radius around the first proposal, and randomly chooses a new set of values within that radius.  
  
  - If this achieves better results than the previous parameters, it is accepted as the new best and the process continues. If the results are worse than the previous value the search procedure may still use this parameter to define further steps. 
  
  - After a given number of iterations, the algorithm stops and provides a list of the best models and their hyperparameters.  

In the algorithm below, we are asking for 50 iterations [but dong at least 100 is recommended].  

```{r cit_grid_result}

set.seed(76544)

cit_grid_result <- tune_sim_anneal( #"tune_sim_anneal()" function for simulated annealing
  object = cit_spec,
  preprocessor = weather_recipe, #preprocessing our data using the recipe
  resamples = resampling_foldcv, #to specify resampling strategy 
  param_info = cit_param, #"cit_param" is the object for CIT engine-specific hyperparameters
  iter = 50 #we are doing 50 iterations to save time in class, but for research we would do at least 100 iterations
)

#install.packages("beepr")
beepr::beep() #for a code chunk that takes a lot of time to run, running "beepr::beep()" plays a beep when the chunk is started, and then finished

cit_grid_result$.metrics[[2]]

```

Notice how we have a column for iterations.  
The first iteration uses a sensible value for the hyper-parameters, and then starts "walking" the parameter space in the direction of greatest improvement.  

Let's collect a summary of metrics (across all folds, for each iteration), and plot them.  

Firs, RMSE (lower is better):  

```{r RMSE}
cit_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = conditional_min_criterion, 
             y = tree_depth 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = factor(mean)),
             size = 3) + 
  scale_color_viridis_d() +
  scale_y_continuous(breaks = seq(1,15,2)) +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "RMSE")
```

We want to minimize the RMSE. The lower the RMSE, the better. In the legend section of the RMSE grid plot seen above, the lowest RMSE is"3.00343891757471" which has a purple color, and we see the purple color dot is on 48 th iteration, which indicates that the lowest RMSE [= "3.00343891757471"] was obtained in the 48 th iteration.

What tree_depth and min criterion values created lowest RMSE?  

tree_depth = 3; conditional_min_criterion = ~ 0.96 [when we have 50 iterations]

Now, let's look into R2 (higher is better):  

```{r R2}
cit_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = conditional_min_criterion, 
             y = tree_depth 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = factor(mean)),
             size = 3) + 
  scale_color_viridis_d() +
  scale_y_continuous(breaks = seq(1,15,2)) +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "R2")

```

We want to see higher R2 (R2: higher is better).

We want to see a maximized R2. The higher the R2, the better. In the legend section of the R2 grid plot seen above, the highest R2 is"0.0795034771578964" which has a yellow color, and we see the yellow colored dot is on 48 th iteration, which indicates that the highest R2 [= "0.0795034771578964"] was obtained in the 48 th iteration.

What tree_depth and min criterion values created highest R2?  

tree_depth = 3; conditional_min_criterion = ~ 0.96 [this was the best result for 50 iterations]

> Previously, we selected the single best model. Now, let's select the best model within one std error of the metric, so we choose a model among the top ones that is more parsimonious.  

```{r}

# Based on lowest RMSE
best_rmse <- cit_grid_result %>%
  select_by_one_std_err("tree_depth", #specifying the hyperparameter for which we want to select the best model within one std error
                        metric = "rmse" #specifying the metric
                        )

best_rmse

```

Based on RMSE, we would choose 
- tree_depth = 14
- conditional_min_criterion = 0.9655238 [notice that p-value = 1 - conditional_min_criterion = 1 - 0.9655238 = 0.0344762]

```{r}

# Based on greatest R2
best_r2 <- cit_grid_result %>%
  select_by_one_std_err("tree_depth", #specifying the hyperparameter for which we want to select the best model within one std error
                        metric = "rsq" #specifying the metric
                        )

best_r2

```

Based on R2, we would choose 
- tree_depth = 14
- conditional_min_criterion = 0.9655238 [notice that p-value = 1 - conditional_min_criterion = 1 - 0.9655238 = 0.0344762]

[Note: It could be that RMSE and R2 give you the same combinations of hyperparameters that optimize both metrics -- it could be the case a lot of times. But it does not happen always -- maybe sometime you will get a differernt combinations of the hyperparameter values for RMSE as compared to R2.]

Based on RMSE, we would choose [Dr. Bastos's got this answer, but I got a different one]     
  - tree_depth = 10     
  - conditional_min_criterion = 0.9926512  

Based on R2, we would choose [Dr. Bastos's got this answer, but I got a different one]     
  - tree_depth = 10  
  - conditional_min_criterion = 0.9926512  

Based on RMSE, we would choose [I got this answer]
- tree_depth = 14
- conditional_min_criterion = 0.9655238

Based on R2, we would choose [I got this answer]
- tree_depth = 14
- conditional_min_criterion = 0.9655238

[Note: It could be that RMSE and R2 give you the same combinations of hyperparameters that optimize both metrics -- it could be the case a lot of times. But it does not happen always -- maybe sometime you will get a differernt combinations of the hyperparameter values for RMSE as compared to R2.]

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}

#At this stage, we will use the fine tuned/ optimized R2 value to fit the model

final_spec <- decision_tree(tree_depth = best_r2$tree_depth) %>% #notice that we are no longer using tree_depth = tune() (because we already fine tuned the values) and will use the fine tuned/ optimized value "tree_depth = best_r2$tree_depth"
  # Specify the engine
  set_engine("partykit",
             conditional_min_criterion = best_r2$conditional_min_criterion) %>% #notice that we are no longer using conditional_min_criterion = tune() (because we already fine tuned the values) and will use the fine tuned/ optimized value "conditional_min_criterion = best_r2$tree_depth"
  # Specifying mode  
  set_mode("regression")
  

final_spec

```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Training the optimum hyperparameter values on the **entire training set**  [not training + testing set; just training set]
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}

final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

#The "last_fit()" function does 2 things for us: 1) it applies the last model that we just fine tuned to the entire training set (which has not happened yet for this CIT exercise) 2) it uses then that model that apply to the entire training set to make predictions on the test set #this is the 1st time that the test set comes back at the very end #Notice that the training set was completely ignored in the training set 

final_fit %>%
  collect_predictions()

#Here when we collect predictions, we have 211 rows because the test set had 211 rows. Notice that we have a column for "strength_gtex" which contains the actual values for "strength_gtex" that were observed, and a ".pred" column that contains the predicted value that this model is giving/predicting

```

The "last_fit()" function does 2 things for us: 

1) it applies the last model that we just fine tuned to the entire training set (which has not happened yet for this CIT exercise). 

2) it uses then that model that apply to the entire training set to make predictions on the test set #this is the 1st time that the test set comes back at the very end.
Notice that the training set was completely ignored in the training set 

Metrics on the **test set**:  

```{r}
final_fit %>%
  collect_metrics()
```
Very v-e-r-y important note to remember: *The fit metrics on the test set really shows the predictive ability of our model.* The fit metrics on the training set just give us an idea of how well the model fit our data. 
 
 Test set fit metrics:
 - RMSE = 3.11734102	
 - R2 = 0.08138124	

Metrics on **train set** (for curiosity and compare to test set):  

```{r}

# RMSE
final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```

 Training set fit metrics:
 - RMSE = 2.8601998		
 - R2 = 0.1492775	

> How does metrics on test compare to metrics on train?  

 Test set fit metrics:
 - RMSE = 3.11734102	
 - R2 = 0.08138124	
 
 Training set fit metrics:
 - RMSE = 2.8601998		
 - R2 = 0.1492775	 

Very important note: We see a larger RMSE on the test set (as compared to training set), and a lower R2 on the test set (as compared to training set) [RMSE: lower the better; R2: higher the better]. That is, we see better fit metrics in the training set than those of the test set. This is very typical, because it would make sense that our model predicting better the data it has already seen during training , 

> You would expect all these ML models to have better fit metrics on the training set (because they already saw the train set data when they were being trained, so they had a chance to learn those patterns), and on the test set which are completely new data, the fit metrics in the test set would not probably be as good as the train set.

Predicted vs. observed plot: [regardless of how simple or complex the machine learning model is, we should always expect to see this "Predicted vs. observed plot"]

```{r}

final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = strength_gtex,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 

```



**TLDR; the strength values of only the test set is along the x-axis, whereas the the predicted values on the y-axis are coming from the (trained) model that was trained on the trained set.** In this Predicted vs. observed plot, we are using the model that we already fit for best hyperparameter values and used those values to apply the model to just on the train set to train the model, and then using that model to make predictions on the test set. Therefore, what we are seeing in the x-axis of the predicted vs. observed plot is the strength values only on the test set, but the predicted values are coming from the (trained) model that was trained on the trained set. 

In this Predicted vs. observed plot, we notice 4 horizontal lines that are parallel to each other [we did not see any such parallel line patterns in the previous elastic net model]. 

> Why the 4 horizontal lines?  

We see 4 horizontal lines on the predicted vs. observed plot because we have 4 terminal nodes in this tree, meaning that all observations are gonna fall within one of these 4 nodes. 

For all of the 4 conditions(/combinations) in this tree (e.g., "mean_vp.pa_May" <= 1576.1 -> Node2; "mean_vp.pa_May" > 1576.1 -> "mean_srad.wm2_Aug" <= 360.6 -> Node 4; "mean_vp.pa_May"  > 1576.1 -> "mean_srad.wm2_Aug" > 360.6 -> "mean_dayl.s_Sep" <= Node 6; "mean_vp.pa_May"  > 1576.1 -> "mean_srad.wm2_Aug" > 360.6 -> "mean_dayl.s_Sep" > 43818.7 -> Node 7 ), the predicted value that the model gives is the mean of the boxplot for each of the terminal nodes. Even though the data distribution of the boxplot is the distribution on the training set data of the variables that fell into a particular condition/combination of the tree, but when we use the model to make a prediction,  it will predict the man of the boxplot for each one of those terminal nodes. That's we see the 4 streaks of parallel lines on the perdicted vs. observed plot.

> How can we get more horizontal lines in this type of algorithm?  

We will get more horizontal lines in this type of algorithm if the number of terminal nodes are more. 


## Variable importance:   

Very important note:

Another plot that is very important to show in machine learning regardless of the type of machine learning model is the "variable importance plot", which for elastic net was the magnitude of the slope for each variable. Elastic net was a linear model with many x variables, and each of these x variables had a slope. So, the larger was the magnitude of the slope of a particular x variable, the greater was the importance of that particular variable in the model. For conditional inference tree, that is not how it works. For conditional inference tree, there is this iterative process for selecting one variable, and then doing a binary split [then again selecting anther variable, and then doing another binary  split]. And variables that come on top are more important than variable that come afterwards.  


```{r}

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather_train)) %>%
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```
We see that only 3 variables show up in the variable importance plot: 
1) mean_vp.pa_May [mean vapor pressure deficit in May]
2) mean_srad.wm2_Aug [mean solar radiation in August]
3) mean_dayl.s_Sep [mean day length in September]

and anything else does not have any value of importance. 

> Why do you think that We see only 3 variables showing up in the variable importance plot?

because those were the only variables that entered the model. None of the other variables entered the model, so they had no importance."mean_vp.pa_May" was the 1st variable to show up in the variable importance plot because it was the 1st variable to enter the model (i.e., it was the most important variable). The variable that comes up on top has more importance than the variables that comes afterwards (i.e., variable importance appear on chronological order in the variable importance plot).


> Tree:  

One of the biggest pros of CIT (conditional inference tree) is that it gives you only 1 tree model that you can look and interpret, which is not gonna be the case for random forest.

```{r recipe baked only on the training set "weather_train"}

final_spec %>% #"final_spec" is where we trained the entire training set
  fit(strength_gtex ~ .,
      data = bake(weather_prep, weather_train) #bake(weather_prep, weather_train): to apply the recipe steps to the  training data #now we bake the entire weather data, which means applying the "recipe" that we had initially applied to the training set
      ) %>%
  .$fit %>% # the "." is for all the outcome before this pipe, we are subsampling the "final_spec" object by pulling out the "fit" from this model #to pull the "fit" component from the "final_spec" object
  plot() #to plots the tree for us

#weather_prep #trained data with the "recipe" (= data preprocessing) applied it
#weather #the main data set that has all the rows (including training and testing data)
  
```
The tree plot for only the training data is seen above. Dr. Bastos said that this is the best practice: to make the tree plot based on the training data.


```{r recipe baked on the entire "weather" data set}

final_spec %>% #"final_spec" is where we trained the entire training set
  fit(strength_gtex ~ .,
      data = bake(weather_prep, weather) #bake(weather_prep, weather): to apply the recipe steps to the entire data (that contains both training and testing data sets) #now we bake the entire weather data, which means applying the "recipe" that we had initially applied to the training set
      ) %>%
  .$fit %>% # the "." is for all the outcome before this pipe, we are subsampling the "final_spec" object by pulling out the "fit" from this model #to pull the "fit" component from the "final_spec" object
  plot() #to plots the tree for us

#weather_prep #trained data with the "recipe" (= data preprocessing) applied it
#weather #the main data set that has all the rows (including training and testing data)
  
```

[ Ask  Dr. Bastos: 
In CIT, what is the standard practice when it comes to baking recipe? Should we bake the recipe to the training data [ bake(weather_prep, weather_train) ], or should we bake the recipe to the whole "weather" data set that contains both the training and testing data  [ bake(weather_prep, weather) ]? ]

The tree plot for the whole "weather" data (that contains both the training and testing data) is seen above. 

Dr. Bastos said that sometimes people like to see the tree plot based on the whole data. But as a best practice, we should use the training data to plot the tree. 

As we can see, the tree plot with only the training data (baked for recipe) had 3 predictor variables in the tree (mean_vp.pa_May [mean vapor pressure deficit in May], mean_srad.wm2_Aug [mean solar radiation in August], mean_dayl.s_Sep [mean day length in September] ), whereas the tree plot with the entire "weather" data baked for recipe had 2 predictor variables in the tree ("mean_srad.wm2_Jul" [mean solar radiation in July], "mean_vp.pa_Jun" [mean vapor pressure deficit in June]

One of the limitations of this model is that CIT model is highly sensitive to the data we use to train it, and therefore we would get a different model whether we are creating the final tree plot just on the train set (= "weather_train"; yields 4 terminal nodes on the tree map), or on the whole (entire) data set (entirety of the training and testing set together = "weather" which yields 3 terminal nodes in ther tree map). Another thing which is very important to notice that the model with only the training data baked with the recipe in it gives (three) different predictor variables (3 predictor variables in the tree: mean_vp.pa_May [mean vapor pressure deficit in May], mean_srad.wm2_Aug [mean solar radiation in August], mean_dayl.s_Sep [mean day length in September] ) that are different than the predictor variables (2 predictor variables in the tree: "mean_srad.wm2_Jul", "mean_vp.pa_jun") with the whole "weather" data set (containing both training and test sets) baked with the recipe in it.

Again: One of the limitations of the CIT model is that CIT model is highly sensitive to the data we use to train it. So, maybe if we come back and collect more data next year and use these data to update the model, it will most likely give a different model in the end (which is a limitation). We do want model that are more robust (that they will get updated and improve over time, but will not switch the model back and forth every time we bring in slightly new data. )  

In other words, we would get a different tree map if we bake the recipe on the training data [ bake(weather_prep, weather_train) ] as compared to the tree map if we bake the recipe on the whole (entire) data set containing both training and testing data sets [ bake(weather_prep, weather) ]. 

> How would you interpret this plot?

The "mean_vp.pa_May" [mean vapor pressure deficit in May] had a higly significant p-value (p < 0.001) and therefore entered the model. And then it was split into 2 categories (<= 1576.1, > 1576.1). If the "mean_vp.pa_May" [mean vapor pressure deficit in May] was greater than 1576.1, we came to the right side on mean_srad.wm2_Aug [mean solar radiation in August]. If it was lower than 1576.1, we came to the left side on "Node 2". 

Then, a new split happened on "mean_srad.wm2_Aug" [mean solar radiation in August] because it had a highly significant p-value (p < 0.001) to further split into 2 categories (<= 360.6, > 360.6). If the mean_srad.wm2_Aug [mean solar radiation in August] was less than 360.6, then we came to the left on "Node 4". If the mean_srad.wm2_Aug [mean solar radiation in August] was more than 360.6, then we came to the right on "mean_dayl.s_Sep" [mean day length in September].

Then, a new split happened on "mean_dayl.s_Sep" [mean day length in September] because it had a highly significant p-value (p < 0.001) to further split into 2 categories (<= 43818.7, > 43818.7). If the "mean_dayl.s_Sep" [mean day length in September] was less than 43818.7, then we came to the left on "Node 6". If the "mean_dayl.s_Sep" [mean day length in September] was more than 43818.7, then we came to the right on "Node 7".

On the y-axis of the terminal node [Node 2, Node 4, Node 6, Node 7] is "strength_gtex". By looking at these boxplots, it seems like the highest mean strength was observed at Node 4 [because the median of the boxplot at Node 4 are higher than the others] when we have "mean_vp.pa_May" [mean vapor pressure deficit in May] greater than 1576.1 and "mean_srad.wm2_Aug" [mean solar radiation in August] less than 360.6. Then, we observed lower mean strength than the boxplot on Node 4 for all other conditions. 

For all of the 4 conditions(/combinations) in this tree (e.g., "mean_vp.pa_May" <= 1576.1 -> Node2; "mean_vp.pa_May" > 1576.1 -> "mean_srad.wm2_Aug" <= 360.6 -> Node 4; "mean_vp.pa_May"  > 1576.1 -> "mean_srad.wm2_Aug" > 360.6 -> "mean_dayl.s_Sep" <= Node 6; "mean_vp.pa_May"  > 1576.1 -> "mean_srad.wm2_Aug" > 360.6 -> "mean_dayl.s_Sep" > 43818.7 -> Node 7 ), the predicted value that the model gives is the mean of the boxplot for each of the terminal nodes. Even though the data distribution of the boxplot is the distribution on the training set data of the variables that fell into a particular condition/combination of the tree, but when we use the model to make a prediction,  it will predict the man of the boxplot for each one of those terminal nodes. That's we see the 4 streaks of parallel lines on the predicted vs. observed plot.

> Why do we have only 3 variables ( mean_vp.pa_May [mean vapor pressure deficit in May], mean_srad.wm2_Aug [mean solar radiation in August], mean_dayl.s_Sep [mean day length in September] ) "variable importance plot"?

Because those were the only variables that entered the model. None of the other variables entered the model, so they had no importance."mean_vp.pa_May" was the 1st variable to show up in the variable importance plot because it was the 1st variable to enter the model (i.e., it was the most important variable). In a CIT model, the variable that comes up on top has more importance than the variables that comes afterwards (i.e., variable importance appear on chronological order in the variable importance plot).

**Therefore, solar radiation in July and vapor pressure in June were the most important variables affecting cotton fiber strength.**  

Greater fiber strength was observed when solar radiation in July was < 398 W/m2 and vapor pressure in June was > 2130 Pa (terminal node 4).    

For a nicer tree visualization, check out the package **ggparty**:  

  - https://github.com/martin-borkovec/ggparty  
  - https://jtr13.github.io/cc19/introduction-to-package-ggparty.html  
  
# Summary  
In this exercise, we covered: 
  - Conditional inference tree algorithm    
  - Set up a ML workflow to train an cit model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used **iterative search** to find the best values for max_depth and min_criterion    
  - Used 10-fold cross validation as the resampling method  [to train the model on the training set by further splitting it into folds, and each fold having a training and assessment set]
  - Used both R2 and RMSE as the metrics to select best model  
  - Once final model was determined, used it to predict **test set** [to explore variable importance, and especially we created and examined the decison tree plot; For random forest, we will not have a tree plot, because random forest does not have a single model to interpret ] 
  - Evaluated it with predicted vs. observed plot, R2 and RMSE metrics, variable importance, and tree plot    
  
## Will come in exam: what are the pros and cons of CIT?
  

