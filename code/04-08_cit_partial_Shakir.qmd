---
title: "Conditional inference tree"
format: html
---

## Will come in exam: What are the pros and cons of CIT?

# Learning objectives  
Our learning objectives are to:  
  - Understand conditional inference tree (cit) algorithm 
  - Use the ML framework to:  
    - pre-process data
    - train a cit model 
    - evaluate model predictability 
  - Explore a few new concepts:  
    - Iterative search with **simulated annealing** (instead of fixed grid)  
    - Selecting **best model within 1 sd**  
    

# Introduction  
As we previously learned, linear regression models can suffer from **multicollinearity** when two or more predictor variables are highly correlated.  

The methods we mentioned to overcome multicollinearity include:  
  - Dimensionality reduction (e.g., PCA)  
  - Variable selection:
    - by hand [field-specific expertise]
    - by models  

Today, we'll explore another model that performs variable selection, but in a different way: **conditional inference trees (CIT)**.

[ What is porting a model? 
Answer: We are training these models here with intentions that we can make predictions. So, if we want to deploy our model on a dashboard to allow other people to upload their data  to get a prediction, that would be porting our model (now that we have trained our model, we want to make it available for other to upload their data and get a prediction -- that would be porting a model; that would be basically we deploying our model) ]

## CIT 
Conditional inference tree is a recursive model that works iteratively performing 2 steps:  
  - **variable selection**  [i.e., selecting a predictor variable in relation to our predicted variable]
  - **binary split**  
  
It performs **variable selection** by first running all possible bivariate models between the response variable (e.g., strength_gtex) and each individual explanatory variable (e.g., sum_precip.mm_June), in the form of **strength_gtex ~ sum_precip.mm_June**. Then, it selects the explanatory variable with the **lowest p-value** as the most important.  

After it selects the most important variable, it performs a **binary split** on that variable, which involves finding a value of the explanatory variable which, if used to split the data in 2 groups, will minimize the error of the two splits.  

After making the first split, it performs a new iteration on each of the splits, performing again variable selection and binary split.  

[Note: If you imagine this tree could grow indefinitely, until we basically have 1 observation per terminal node (where the tree stops growing -- the bottom nodes with boxplots), this tree can be very very very complex, which will create an issue of overfitting. So we need to find a way to allow this tree to grow just enough, but not excessively enough, we don't want it to overfit. Remember the bias-variance trade-off? This is a model that is very easy to allow it to be too complex. We want to make sure that it's complex enough, but not more complex than needed.]

The tree stops growing (stops iterating) when it reaches a given stopping criteria, as for example, maximum tree depth (= how many layers the tree is gonna have).    


Let's look into how it works, with figures.  

![](https://ars.els-cdn.com/content/image/1-s2.0-S0378429021002331-gr5.jpg)
[Very important note: In conditional inference tree, Whatever grain yield data point on the terminal node that matches a specific predictor variable conditions, if we were to make a prediction based on this model, the prediction will be the mean of the boxplot of that terminal node regardless the yield were higher or lower. As long as the predictor variables match that pattern, the prediction that the model is gonna give you is the mean of the boxplot of that particular terminal node. It's always gonna give you the same predictions even if our predictor variables are slightly changing but still matching the same internal conditions. ]

Terminology:  
  - **Root node**: node on top, with all observations (Rain_Cum)    
  - **Internal node**: all intermediate nodes (e.g., Flag_Leaf_Fungi)    
  - **Leaf/terminal node**: the bottom nodes with the boxplots  
  
Variables selected first (on top) are more important than variables selected afterwards. In this example, the most important variable in explaining grain yield is **Rain_Cum**.  c  

[Very important note:
Conditional inference tree model is not prone to multicollinearity because even though it is a multivariate analysis, it is doing variable selection on a bivariate relationship. So, it runs all bivariate relationships at each step and never runs a multivariate model in the sense that the predicted variable (e.g., fiber strength) is explained by all the predictor variables (e.g., all 73 weather variables). Instead, the predicted variable (fiber strength) is explained by each individual predictor variable separately, choosing the one with the lowest p-value, and then doing binary split. That is how this Conditional inference tree model remedies multicollinearity. Even though the ensamble of it is a multivariate model, but the individual pieces of the tree are all derived from a bivariate relationship.]

[Is having too many internal/terminal nodes a good thing, or bad thing?
Answer: bad thing, because it will lead to overfitting]

## Creating partitions  
Let's look into a simpler example where we are predicting y as a function of x.  

A simple CIT model from this relationship would be with a single break:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/decision-stump-1.png)

If we make a plot of y ~ x and show the split above along the x axis, this is how it would look like:  

![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/decision-stump-2.png)

We can build a more complex tree by allowing it to be deeper:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/depth-3-decision-tree-1.png)

Which will translate into more breaks along the x-axis of the scatterplot:  

![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/depth-3-decision-tree-2.png)

We can allow it to be VERY complex:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/deep-overfit-tree-1.png)

With the following scatterplot breakpoints:  
![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/deep-overfit-tree-2.png)

So, how can we control the simplicity/complexity of the tree?  

**Training a model by fine-tuning its hyper-parameters**.

There will be 2 main hyperparameters that we will fine-tune:  
  - **maximum depth**: maximum (vertical) depth of the tree (i.e., maximum layer of the tree i.e., how many horizontal layers are stuck on top on each other)
  - **minimum criterion**:  the value of (1 - p-value) that must be exceeded in order to implement a split
     
## Pros vs. cons of CIT  
Pros:  
  - Non-parametric [does not assume any specific distribution of data i.e., no need to satisfy the assumptions of residual normality, independence, equal variance] 
  - It can model non-linear relationships [It can model both linear and non-linear relationships] 
  - The model created is a decision tree, very easy to interpret [One of the biggest pros of CIT (conditional inference tree) model is that it is creating 1 single tree that is highly interpretable. It's a very simple model, and highly interpretable. A lot of time there is a trade-off between a model complexity and model interpretability. Models that are really complex, they may be the best at predicting, but they can also be very difficult to interpret. With CIT (conditional inference tree), the model is really simple so it is easy to interpret.]  
  - Can be used with both numerical and categorical response variables  [as compared to elastic net, which works with only numerical response variable]
  
Cons:  
  - Can have higher bias [very prone to overfitting, but we have to try our best to train it]  
  - Potentially lower predictive power: any observation matching a given condition will be predicted as the mean of the terminal node. [Although on the training set, we see the variability (expressed as error caps) around each of the boxplot at the terminal node, but if we use this model for prediction, each of those conditions will give you a prediction of the *mean* of the boxplot, basically on the prediction it ignores the variability that we see around the error caps/bars of the boxplot]   
  
    
# Setup  
```{r}
#| message: false
#| warning: false

#install.packages("partykit") #one of the packages that implements conditional inference tree on R (there are more packages)
#install.packages("finetune") #to fine tune hyperparameters specific to conditional inference tree model
#install.packages("bonsai") #to fine tune hyperparameters specific to conditional inference tree model

library(tidymodels)
library(tidyverse)
library(vip)
library(partykit)
library(finetune)
library(bonsai)
```

```{r weather}
weather <- read_csv("../data/weather_monthsum.csv")

weather
```

# ML workflow  
We're going to use the same workflow as we used for elastic net.   

## 1. Pre-processing  
Here's where we perform **data split** and **data processing**.  

### a. Data split  
For data split, let's use **70% training / 30% testing**.

```{r weather_split}
# Setting seed to get reproducible results  
set.seed(931735)

# Setting split level  
weather_split <- initial_split(weather, 
                               prop = .7,
                               strata = strength_gtex #to do stratified sampling on "strength_gtex" to make sure that we have a similar distribution of predicted variable across training and testing 
                               )

weather_split
```


```{r weather_train}
# Setting train set 
weather_train <- training(weather_split)

weather_train
```
How many observations?

```{r weather_test}
# Setting test split
weather_test <- testing(weather_split)

weather_test
```
How many observations?  

Let's check the distribution of our predicted variable **strength_gtex** across training and testing: 
```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = strength_gtex),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = strength_gtex),
               color = "blue") 
  
```

Now, we put our **test set** aside and continue with our **train set** for training.  

  
### b. Data processing  
Before training, we may need to perform some processing steps, like  
  - normalizing    
  - **removing unimportant variables**  
  - dropping NAs  
  - performing PCA on the go  
  - removing columns with single value  
  - others?  

For that, we'll create a **recipe** of these processing steps. 

This recipe will then be applied now to the **train data**, and easily applied to the **test data** when we bring it back at the end.

Creating a recipe is as easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.  

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

Different model types require different processing steps.  
Let's check what steps are required for an elastic net model (linear_reg).
We can search for that in this link: https://www.tmwr.org/pre-proc-table  [in the Table A.1, we see that the decision_tree() does not require any preprocessing methods (e.g., dummy, zv, impute, decorrelate, normalize, transform). So, we don't have to do any of these preprocessing steps. However, we still want to remove the predictor variables from our data set that are outside of the growing season of cotton. Notice that pre-processing steps differ from model to model, so "recipe' steps will be model-specific.]

> Differently from elastic net, variables do not need to be normalized in conditional inference tree, so we'll skip this step


```{r weather_recipe}
weather_recipe <-
  # Defining predicted and predictor variables
  recipe(strength_gtex ~ ., # . is used to indicate all predictor variables should be included in the model (we have 73 predictor variables in this data set) #everything on the left of ~ is/are outcome(s), and everything in the right side of ~ are predictors
         data = weather_train) %>% #note that we are using the training data in this recipe
  # Removing year and site  
    step_rm(year, site, matches("Jan|Feb|Mar|Apr|Nov|Dec")) #%>% #rm in step_rm() stands for remove #although we could have used dplyr::select, but we used step_rm() because it is within the scope of "recipes" #we want to remove year and site from our training set #matches("Jan|Feb|Mar|Apr|Nov|Dec"): to remove weather variables that represent data outside of the growing season: Jan, Feb, Mar, Apr, Nov, Dec  #matches("Jan|Feb|Mar|Apr|Nov|Dec"): to remove all columns that have these months (Jan, Feb, Mar, Apr, Nov, Dec) as part of their (column) names
  # Normalizing all numeric variables except predicted variable
  #step_normalize(all_numeric(), -all_outcomes())

weather_recipe
```

Now that we have our recipe ready, we **need to apply it** to the training data in a process called prepping:

```{r weather_prep}
weather_prep <- weather_recipe %>%
  prep() #prep(): to prepare the recipe

weather_prep
```


Now, we're ready to start the model training process!

## 2. Training  
### a. Model specification  
First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use [regression (when response/predicted y variable is numerical) or classification (when response/predicted y variable is categorical)] 

> Elastic nets can only be run for a numerical response variable. CITs can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

Conditional inference tree **hyperparameters**:  
  - **tree_depth**: maximum depth of the tree (i.e., maximum layer of the tree; so we are basically allowing the tree to grow to different depths, and see which depth is gonna give us the best model)   
  - **mincriterion**: the value of (1 - p-value) that must be exceeded in order to implement a split  (Let's say, we only want to allow variabels having p-value less than 0.05 to enter the model, then our mincriteroin would be (1 - 0.05) = 0.95 [inverse of p-value] that must be exceeded in order to implement a split)
    - **min_n**: minimum number of data points in a node that are required for the node to be split further [Let's say, we are training our tree, and there is an internal node that has 3 variables in the internal node. If our "min_n" is 5, that node is not allowed to split anymore.]   

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **partykit** engine/package.  

[Note: We determine the model type, then we determine the engine, and (for cit) then we determine the mode. What we mean by engine here is that even though we have a given model type (cit, in this case), there are different packages in R that will do the same analysis. For example, if we use another package called "rpart", that package is very similar to "partykit" (which we are using here), uses R2 as the metric to select variables rather than p-value (as in partykit). So, there are some differences on what fit metrics (e.g., p-value, or R2) each package use to conduct variable selection. In this case, we are using "partykit" engine.]        
  
```{r cit_spec}

cit_spec  <- 
  # Specifying cit as our model type, asking to tune the hyperparameters
  decision_tree(tree_depth = tune()) %>% #tree depth is the hyperparameter we are gonna fine tune, and because we are gonna fine tune, we are not gonna mention any number inside the "tune()" function as a placeholder to tell the training step of this process is to say that hey, we want to fine tune "tree_depth". If we indicated any particular value inside "tune()", it would not fine tune, and would just use that value. E.g., if we indicated 10, it would use 10
    # Specify the engine
  set_engine("partykit", #to specify what package we want to use for the implementation of CIT
             conditional_min_criterion = tune()
             ) %>%
    # Specifying mode
  set_mode("regression") #because CIT can handle both numerical and categorical type of predicted/response variable, we need to specify the mode as "regression" since our predicted/response variable is numerical

#One thing that is different in CIT from elastic net is that the model type was linear_reg() [for linear regression], and both of the hyperparameters that we were fine tuning (penalty, mixture) were at the level of fine type function (i.e., inside linear_regression() function). But for CIT, the "conditional_min_criterion" hyperparameter are specific to the "partykit" engine, because of that we will tell the model to fine tune "conditional_min_criterion" hyperparameter at the set_engine() level instead of the model type level (i.e., inside "decision_tree()" function) as we did in the elastic net
#that's why we need to fine tune this"conditional_min_criterion" hyperparameter inside the set_engine() function by using the "conditional_min_criterion" argument as "conditional_min_criterion = tune()"

cit_spec

```

Notice how the main arguments above do not have a value **yet**, because they will be tuned.  

> Notice how we have one hyperparameter at the level of decision tree (tree_depth) and another one at the level of the engine (min_criterion).  

### b. Hyper-parameter tuning  
> On our previous exercise, we used a fixed grid search approach. This time, let's use an iterative search approach.

For our iterative search, we need:  
  - Our model specification (`cit_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet)  
  - **Parameter information** (don't have yet)      
  
[Resampling strategy is basically on your train set -- we use only the train set to find the best combinations of hyperparameters. To do that, we have to further split our train set into analysis set (for training) and assessment set (for validation set) through resmapling methods (e.g., v-fold cross-validation). For each resample, models with given hyperparameter values are fit independently on the analysis set and evaluated on the assessment set. Some common resampling methods are:  V-fold cross-validation, Leave-one-out cross-validation, Bootstrapping 

Let's define our resampling strategy below, using a (v-fold cross-validation, with v = 10) 10-fold cross validation approach:  

```{r resampling_foldcv}
set.seed(34549)
resampling_foldcv <- vfold_cv(weather_train, 
                              v = 10)

resampling_foldcv
resampling_foldcv$splits[[1]]
resampling_foldcv$splits[[2]]
```

In the output, we have 10 folds. The whole training data was split into 10 parts, then each one of these folds have 9 parts for training and 1 part for testing. And every single fold has a different part testing i.e., whatever testing part was on the 1st fold, it goes back to the other 9 folds, and 1 of the other 0 folds comes back out to the 2nd fold, and so on.   

On the output for the 1st fold ( resampling_foldcv$splits[[1]] ), notice that 487 is the total number of observations on the 1st fold which is the total number of data in our training set. And 1st fold has 438 rows (of those 487 total rows) are on the (analysis/) training and 49 are on the assessment. Then it does this for all the 10 folds.  

On each fold, we'll use **438** observations for training and **49** observations to assess performance.    

Now, let's define our parameter information.  

We need to create this object because engine-specific hyperparameters have a missing object, and it is required to be not missing to perform simulated annealing.  

```{r cit_param}

#For CIT, we have to do this extra step so we can fine tune the CIT engine (= "partykit") specific hyperparameters: (1) tree_depth , (2) mincriterion, (3) min_n

#tree_depth (maximum depth of the tree i.e., maximum layer of the tree; so we are basically allowing the tree to grow to different depths, and see which depth is gonna give us the best model), mincriterion (the value of (1 - p-value) that must be exceeded in order to implement a split), min_n (minimum number of data points in a node that are required for the node to be split further)

cit_param <- cit_spec %>%
  extract_parameter_set_dials() %>%
  update(conditional_min_criterion = #"update()" comes from "recipes"
           conditional_min_criterion()) #to set the nparam[+] for the "conditional_min_criterion" which was missing up to the previous pipe #setting for the specification of the model, what are the ranges of the hyperparameters that will be finetuned for each one of those hyperparameters

cit_param 

cit_param$object #We can see what is the range that the model is gonna train for us #From the output, we see that "tree depth" is gonna test for 1 to 15 [i.e., having a tree depth of 1 layer only, to all the way to 15 layers; that's the default it is gonna test for us.]; "Transformer: prob-logis [0, 1]" means that the "conditional_min_criterion" is going from 0 to 1 

```

Now, let's perform the search below.  

[For elastic net, we created a fixed grid at this stage. But for CIT, we will do simulated annealing.]

We will use an iterative search algorithm called **simulated annealing**.  

Here's how it works:  
![](https://www.tmwr.org/figures/iterative-neighborhood-1.png)
  - In the example above, mixture and penalty from an elastic net model are being tuned.  

  - It finds a candidate value of hyperparameters and their associated rmse to start (iteration 1).  

  - It establishes a radius around the first proposal, and randomly chooses a new set of values within that radius.  
  
  - If this achieves better results than the previous parameters, it is accepted as the new best and the process continues. If the results are worse than the previous value the search procedure may still use this parameter to define further steps. 
  
  - After a given number of iterations, the algorithm stops and provides a list of the best models and their hyperparameters.  

In the algorithm below, we are asking for 50 iterations [but dong at least 100 is recommended].  

```{r cit_grid_result}

set.seed(76544)

cit_grid_result <- tune_sim_anneal( #"tune_sim_anneal()" function for simulated annealing
  object = cit_spec,
  preprocessor = weather_recipe, #preprocessing our data using the recipe
  resamples = resampling_foldcv, #to specify resampling strategy 
  param_info = cit_param, #"cit_param" is the object for CIT engine-specific hyperparameters
  iter = 50
)

cit_grid_result$.metrics[[2]]

```

Notice how we have a column for iterations.  
The first iteration uses a sensible value for the hyper-parameters, and then starts "walking" the parameter space in the direction of greatest improvement.  

Let's collect a summary of metrics (across all folds, for each iteration), and plot them.  

Firs, RMSE (lower is better):  

```{r RMSE}
cit_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = conditional_min_criterion, 
             y = tree_depth 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = factor(mean)),
             size = 3) + 
  scale_color_viridis_d() +
  scale_y_continuous(breaks = seq(1,15,2)) +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "RMSE")
```

We want to minimize the RMSE (RMSE: lower is better).

What tree_depth and min criterion values created lowest RMSE?  

Now, let's look into R2 (higher is better):  

```{r R2}
cit_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = conditional_min_criterion, 
             y = tree_depth 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = factor(mean)),
             size = 3) + 
  scale_color_viridis_d() +
  scale_y_continuous(breaks = seq(1,15,2)) +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "R2")

```

We want to see higher R2 (R2: higher is better).

> Previously, we selected the single best model. Now, let's select the best model within one std error of the metric, so we choose a model among the top ones that is more parsimonious.  

```{r}

# Based on lowest RMSE
best_rmse <- cit_grid_result %>%
  select_by_one_std_err("tree_depth",
                        metric = "rmse"
                        )

best_rmse

```

```{r}

# Based on greatest R2
best_r2 <- cit_grid_result %>%
  select_by_one_std_err("tree_depth",
                        metric = "rsq"
                        )

best_r2

```
Based on RMSE, we would choose [Dr. Bastos's got this answer, but I got a different one]     
  - tree_depth = 10     
  - conditional_min_criterion = 0.9926512  

Based on R2, we would choose [Dr. Bastos's got this answer, but I got a different one]     
  - tree_depth = 10  
  - conditional_min_criterion = 0.9926512  

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}

final_spec <- decision_tree(tree_depth = best_r2$tree_depth) %>%
  # Specify the engine
  set_engine("partykit",
             conditional_min_criterion = best_r2$conditional_min_criterion) %>%
  # Specifying mode  
  set_mode("regression")
  

final_spec

```

## 3. Validation  
Now that we determined our best model, let's do our **last fit**.

This means 2 things:  
  - Training the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}

final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()

```



Metrics on the **test set**:  

```{r}
final_fit %>%
  collect_metrics()
```
 The fit metrics on the test set really shows the predictive ability of our model.

Metrics on **train set** (for curiosity and compare to test set):  

```{r}

# RMSE
final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(strength_gtex, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(strength_gtex ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(strength_gtex, .pred)
    
  )

```

How does metrics on test compare to metrics on train?  

Predicted vs. observed plot:   

```{r}

final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = strength_gtex,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 

```

Why the 4 horizontal lines?  

How can we get more horizontal lines in this type of algorithm?  

Variable importance:  

```{r}

final_spec %>%
  fit(strength_gtex ~ .,
         data = bake(weather_prep, weather_train)) %>%
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```


Tree:  

```{r}

final_spec %>%
  fit(strength_gtex ~ .,
      data = bake(weather_prep, weather_train) #bake(weather_prep, weather): applies recipe to the entire training data
      ) %>%
  .$fit %>%
  plot() #plots the tree for us

#weather_prep #trained data with recipe included in it
#weather
  
```

**Therefore, solar radiation in July and vapor pressure in June were the most important variables affecting cotton fiber strength.**  

Greater fiber strength was observed when solar radiation in July was < 398 W/m2 and vapor pressure in June was > 2130 Pa (terminal node 4).    

For a nicer tree visualization, check out the package **ggparty**:  

  - https://github.com/martin-borkovec/ggparty  
  - https://jtr13.github.io/cc19/introduction-to-package-ggparty.html  
  
# Summary  
In this exercise, we covered: 
  - Conditional inference tree algorithm    
  - Set up a ML workflow to train an cit model  
  - Used `recipes` to process data
  - Used `rsamples` to split data  
  - Used **iterative search** to find the best values for mas_depth and min_criterion    
  - Used 10-fold cross validation as the resampling method  
  - Used both R2 and RMSE as the metrics to select best model  
  - Once final model was determined, used it to predict **test set**  
  - Evaluated it with predicted vs. observed plot, R2 and RMSE metrics, variable importance, and tree plot    
  
## Will come in exam: what are the pros and cons of CIT?
  

